diff --git a/src/training/train_rm_scand.py b/src/training/train_rm_scand.py
index 04dcef7..d175c92 100644
--- a/src/training/train_rm_scand.py
+++ b/src/training/train_rm_scand.py
@@ -5,6 +5,7 @@ import sys
 import os
 import time
 import datetime
+import wandb
 from torch.utils.tensorboard import SummaryWriter
 
 sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
@@ -12,26 +13,28 @@ sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
 from torch.utils.data import DataLoader, random_split
 from data.scand_pref_dataset import SCANDPreferenceDataset
 from utils.reward_model_scand import RewardModelSCAND
+from utils.reward_model_scand_2 import RewardModelSCAND2
+
 from utils.plackett_luce_loss import PL_Loss
 
 # user defined params;
 project_name = "Offline-IRL"
 exp_name = "SCAND_test"
 h5_file = "/fs/nexus-scratch/gershom/IROS25/Datasets/scand_preference_data.h5"
-checkpoint_dir = "/fs/nexus-scratch/gershom/IROS25/Offline-IRL/models/checkpoints"
-BATCH_SIZE = 32 # 64 = 12GB VRAM, 32 = 6.9GB VRAM
-LEARNING_RATE = 3e-4
+checkpoint_dir = "/fs/nexus-scratch/gershom/IROS25/Offline-IRL/src/models/checkpoints"
+BATCH_SIZE = 256 
+LEARNING_RATE = 5e-5
 NUM_QUERIES = 4
 HIDDEN_DIM = 768
-N_EPOCHS = 10
+N_EPOCHS = 200
 train_val_split = 0.8
 num_workers = 4
-batch_print_freq = 10
-gradient_log_freq = 100
+batch_print_freq = 5
+gradient_log_freq = 50
 notes = "implementing wandb"
 use_wandb = True
 save_model = True
-device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
 print(f"Using device: {device}")
 
 # Load Dataset and Split
@@ -54,14 +57,15 @@ run_config = {
     "save_model": save_model,
     "batch_print_freq": batch_print_freq,
 }
+
 # Get the current time
 now = datetime.datetime.now()
 
 # Format the time as a string
 timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
 run_name = f"{exp_name}__{timestamp}"
+
 if use_wandb:
-    import wandb
     wandb.init(
         project=project_name,
         notes=notes,
@@ -70,6 +74,7 @@ if use_wandb:
         name=run_name,
         save_code=True,
     )
+    
 writer = SummaryWriter(f"runs/{run_name}")
 writer.add_text(
     "hyperparameters",
@@ -77,22 +82,43 @@ writer.add_text(
 )
 
 # Define Model, Loss, Optimizer
-model = RewardModelSCAND(num_queries=NUM_QUERIES, hidden_dim=HIDDEN_DIM).to(device)
+model = RewardModelSCAND(num_queries=NUM_QUERIES).to(device)
 criterion = PL_Loss()
 optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
-scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
+scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)
+
+# Load from latest checkpoint (if available)
+latest_checkpoint = None
+if os.path.exists(checkpoint_dir):
+    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(".pth")]
+    if checkpoint_files:
+        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split("_")[-1].split(".")[0]))  # Find latest checkpoint
+        latest_checkpoint_path = "/fs/nexus-scratch/gershom/IROS25/Offline-IRL/src/models/checkpoints/model_epoch_40.pth"
+        checkpoint = torch.load(latest_checkpoint_path, map_location=device)
+        
+        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
+        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
+        start_epoch = checkpoint['epoch']
+        print(f"Loaded checkpoint from {latest_checkpoint_path} at epoch {start_epoch}")
+    else:
+        start_epoch = 0
+        print("No previous checkpoint found. Starting fresh.")
+else:
+    start_epoch = 0
+    print("Checkpoint directory does not exist. Starting fresh.")
+
 global_step = 0
 start_time = time.time()
-if use_wandb:
-    if gradient_log_freq > 0:
-        wandb.watch(model, log_freq=gradient_log_freq)
 
-for epoch in range(N_EPOCHS):
+if use_wandb and gradient_log_freq > 0:
+    wandb.watch(model, log_freq=gradient_log_freq)
+
+# Training Loop
+for epoch in range(start_epoch, N_EPOCHS):  # Start from checkpointed epoch
     model.train()
     train_loss = 0.0
     batch_count = 0
 
-    # Training Loop (Per Batch Logging)
     for batch in train_loader:
         images = batch["images"].to(device)
         goal_distance = batch["goal_distance"].to(device)
@@ -105,15 +131,13 @@ for epoch in range(N_EPOCHS):
         optimizer.zero_grad()
 
         # Forward Pass
-        predicted_rewards = model(images, goal_distance, heading_error, velocity, omega, past_action, current_action, batch_size=len(images))  # (batch_size, 25)
+        predicted_rewards = model(images, goal_distance, heading_error, velocity, omega, past_action, current_action, batch_size=len(images))
 
         # Compute Loss
         loss = criterion(predicted_rewards)
 
         # Backpropagation
         loss.backward()
-        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
-
         optimizer.step()
 
         train_loss += loss.item()
@@ -121,9 +145,9 @@ for epoch in range(N_EPOCHS):
         batch_count += 1
         global_step += 1
 
-        if batch_count % batch_print_freq == 0:  # Log every 10 batches
+        if batch_count % batch_print_freq == 0:
             SPS = global_step / (time.time() - start_time)
-            print(f"Epoch [{epoch+1}/{N_EPOCHS}] | Batch {batch_count} | Train Loss: {loss.item():.4f}, steps per second: {SPS:.3f}")
+            print(f"Epoch [{epoch+1}/{N_EPOCHS}] | Batch {batch_count} | Train Loss: {loss.item():.4f}, steps per second: {SPS:.3f} | LR: {optimizer.param_groups[0]['lr']}")
             writer.add_scalar("charts/SPS", SPS, global_step)
             writer.add_scalar("epoch", epoch, global_step)
 
@@ -131,7 +155,7 @@ for epoch in range(N_EPOCHS):
     writer.add_scalar("charts/avg_train_loss", avg_train_loss, global_step)
     writer.add_scalar("epoch", epoch, global_step)
 
-    # Validation Loop (At End of Each Epoch)
+    # Validation Loop
     model.eval()
     val_loss = 0.0
     with torch.no_grad():
@@ -174,4 +198,4 @@ for epoch in range(N_EPOCHS):
         print(f"Checkpoint saved: {checkpoint_path}")
 
 print("Training Complete!")
-writer.close()
\ No newline at end of file
+writer.close()
diff --git a/src/utils/__pycache__/reward_model_scand.cpython-310.pyc b/src/utils/__pycache__/reward_model_scand.cpython-310.pyc
index 377cdbf..5c18290 100644
Binary files a/src/utils/__pycache__/reward_model_scand.cpython-310.pyc and b/src/utils/__pycache__/reward_model_scand.cpython-310.pyc differ
diff --git a/src/utils/reward_model_scand.py b/src/utils/reward_model_scand.py
index f4f8620..599acc4 100644
--- a/src/utils/reward_model_scand.py
+++ b/src/utils/reward_model_scand.py
@@ -29,6 +29,14 @@ class SinusoidalPositionalEncoding(nn.Module):
         """
         return x + self.pe[:, :x.size(1), :].to(x.device)
 
+class ScaledTanh(nn.Module):
+    def __init__(self, alpha=1.0):  # Default alpha=1 (normal tanh)
+        super().__init__()
+        self.alpha = alpha
+
+    def forward(self, x):
+        return torch.tanh(self.alpha * x)
+    
 # Reward Model
 class RewardModelSCAND(nn.Module):
     def __init__(self, num_queries=4):  # Multi-query support
@@ -97,7 +105,7 @@ class RewardModelSCAND(nn.Module):
             nn.Linear(256, 128),
             nn.GELU(),
             nn.Linear(128, 1),
-            nn.Tanh()  # Normalize output range
+            ScaledTanh(alpha=2.0)  # Normalize output range
         )
 
         self._initialize_weights()
diff --git a/src/utils/reward_model_scand_2.py b/src/utils/reward_model_scand_2.py
index 1bbe98a..b772ddf 100644
--- a/src/utils/reward_model_scand_2.py
+++ b/src/utils/reward_model_scand_2.py
@@ -1,8 +1,9 @@
 import torch
 import torch.nn as nn
+from torch.nn import TransformerEncoder, TransformerEncoderLayer
 from transformers import Dinov2Model
 
-# May not use this approach. Can have a variant and train the Query based attn pooling
+# Positional Encoding Class
 class SinusoidalPositionalEncoding(nn.Module):
     def __init__(self, d_model, max_len=256):
         """
@@ -28,49 +29,31 @@ class SinusoidalPositionalEncoding(nn.Module):
         """
         return x + self.pe[:, :x.size(1), :].to(x.device)
 
-class QueryBasedAttentionPooling(nn.Module):
-    def __init__(self, hidden_dim, max_patches=256):
+# Reward Model
+class RewardModelSCAND2(nn.Module):
+    def __init__(self, num_queries=8, num_self_attention_layers = 8):  # Multi-query support
         super().__init__()
-        self.query = nn.Parameter(torch.randn(1, 1, hidden_dim))  # Learnable query vector
-        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, batch_first=True)
-        
-        # Sinusoidal Positional Encoding
-        self.positional_encoding = SinusoidalPositionalEncoding(hidden_dim, max_patches)
-
-    def forward(self, patch_embeddings):
-        """
-        patch_embeddings: (batch_size, num_patches, hidden_dim)
-        Returns: (batch_size, hidden_dim) - Dynamically pooled feature vector
-        """
-        # Add positional encoding to patches
-        patch_embeddings = self.positional_encoding(patch_embeddings)
 
-        batch_size = patch_embeddings.shape[0]
-        q = self.query.expand(batch_size, -1, -1)  # Expand query to batch size
-        attn_output, _ = self.attn(q, patch_embeddings, patch_embeddings)  # Self-attention
-        return attn_output.squeeze(1)  # Remove query dimension
+        self.hidden_dim = 768  # DINOv2 feature size
+        self.num_queries = num_queries  # Number of state queries
 
-class RewardModel(nn.Module):
-    def __init__(self, use_dinov2=True):
-        super(RewardModel, self).__init__()
+        # Load DINOv2
+        self.vision_model = Dinov2Model.from_pretrained("facebook/dinov2-base")
+        self.vision_dim = self.hidden_dim
+        self.num_self_attention_layers = num_self_attention_layers
+        # Freeze DINOv2 weights
+        for param in self.vision_model.parameters():
+            param.requires_grad = False
 
-        self.use_dinov2 = use_dinov2
-        self.hidden_dim = 768  # DINOv2 feature size
+        # Positional Encoding
+        self.positional_encoding = SinusoidalPositionalEncoding(self.hidden_dim)
 
-        # Load DINOv2 (Pretrained)
-        if use_dinov2:
-            self.vision_model = Dinov2Model.from_pretrained("facebook/dinov2-base")
-            self.vision_dim = self.hidden_dim
-        else:
-            self.vision_model = None
-            self.vision_dim = 0  # No image features if not using vision
+        # LayerNorm for Patch Embeddings (Before and After Self-Attention)
+        self.patch_norm = nn.LayerNorm(self.hidden_dim)  
 
-        # Self-Attention Over Patch Embeddings
+        # Self-Attention Over Vision Features
         self.attn_layer = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=8, batch_first=True)
-
-        # Query-Based Attention Pooling (with Positional Encoding)
-        self.attention_pooling = QueryBasedAttentionPooling(self.hidden_dim)
-        self.norm = nn.LayerNorm(self.hidden_dim)
+        self.attn_norm = nn.LayerNorm(self.hidden_dim)
 
         # MLP for numerical inputs (goal distance, heading error, velocity, past and current action)
         self.state_mlp = nn.Sequential(
@@ -78,55 +61,129 @@ class RewardModel(nn.Module):
             nn.ReLU(),
             nn.Linear(128, 256),
             nn.ReLU(),
-            nn.Linear(256, self.hidden_dim)  # No ReLU here
+            nn.Linear(256, 512),
+            nn.ReLU(),
+            nn.Linear(512, self.hidden_dim)
         )
         self.state_norm = nn.LayerNorm(self.hidden_dim)
 
-        # Cross-Attention for Fusion
-        self.cross_attention = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=8, batch_first=True)
-        self.norm = nn.LayerNorm(self.hidden_dim)  # Normalize fused features
+        # Multi-Query Learnable Queries
+        self.state_query_proj = nn.ModuleList([
+            nn.Linear(self.hidden_dim, self.hidden_dim) for _ in range(self.num_queries)
+        ])
+
+        # Cross-Attention Layers
+        self.cross_attention = nn.MultiheadAttention(
+            embed_dim=self.hidden_dim, 
+            num_heads=8, 
+            batch_first=True  # Ensures input is (batch_size, seq_len, hidden_dim)
+        )
+
+        self.cross_attention_norm = nn.LayerNorm(self.hidden_dim)
+
+        self.self_attention_layers = nn.ModuleList([
+            nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=8, batch_first=True)
+            for _ in range(self.num_self_attention_layers)
+        ])
+        self.attention_norms = nn.ModuleList([
+            nn.LayerNorm(self.hidden_dim)
+            for _ in range(self.num_self_attention_layers)
+        ])
+
+
+        self.fusion_norm = nn.LayerNorm(self.hidden_dim)  # Normalize after fusion
+
+        # **MLP-based Query Fusion**
+        self.query_fusion_mlp = nn.Sequential(
+            nn.Linear(self.num_queries * self.hidden_dim, self.hidden_dim),
+            nn.ReLU(),
+            nn.Linear(self.hidden_dim, self.hidden_dim),  # Output fused representation
+            nn.LayerNorm(self.hidden_dim)  # Normalize fused representation
+        )
 
         # Reward Prediction Head
         self.reward_head = nn.Sequential(
+            nn.Linear(self.hidden_dim, 512),
+            nn.Dropout(0.1), 
+            nn.ReLU(),  # Replace ReLU with GELU
+            nn.Linear(512, 256),
             nn.ReLU(),
-            nn.Linear(self.hidden_dim, 128),
+            nn.Linear(256, 128),
             nn.ReLU(),
-            nn.Linear(128, 1)  # Output: Reward Score
+            nn.Linear(128, 1),
+            # nn.Tanh()  # Normalize output range
         )
 
-    def forward(self, image, goal_distance, heading_error, velocity, past_action, current_action, batch_size):
+        self._initialize_weights()
+
+    def _initialize_weights(self):
+        """Initializes weights using Xavier uniform distribution."""
+        for module in self.modules():
+            if isinstance(module, nn.Linear):
+                nn.init.xavier_uniform_(module.weight)
+                if module.bias is not None:
+                    nn.init.zeros_(module.bias)  # Zero-bias initialization
+
+    def forward(self, image, goal_distance, heading_error, velocity, omega, last_action, preference_ranking, batch_size):
         """
         image: (batch_size, 3, 224, 224)
         goal_distance: (batch_size, 1)
         heading_error: (batch_size, 1)
-        velocity: (batch_size, 2)
-        past_action: (batch_size, 2)
-        current_action: (batch_size, 2)
+        velocity: (batch_size, 1)
+        omega: (batch_size, 1)
+        last_action: (batch_size, 2)
+        preference_ranking: (batch_size, 25, 2)  # 25 ranked action pairs
         """
-
+        
         # Extract vision features (Patch embeddings, excluding CLS)
-        if self.use_dinov2:
-            patch_embeddings = self.vision_model(image).last_hidden_state[:, 1:, :]  # Shape: (batch_size, num_patches, hidden_dim)
-        else:
-            patch_embeddings = torch.zeros((batch_size, 0, self.hidden_dim), device=goal_distance.device)
+        patch_embeddings = self.vision_model(image).last_hidden_state[:, 1:, :]  # Shape: (batch_size, num_patches, hidden_dim)
+        patch_embeddings = self.positional_encoding(patch_embeddings)  # Shape: (batch_size, num_patches, hidden_dim)
+        patch_embeddings = self.patch_norm(patch_embeddings)  # Normalizing before patch embeddings
 
-        # Apply Self-Attention on Patch Features
-        patch_embeddings = self.positional_encoding(patch_embeddings)
+        # Self-Attention on Vision Features
         attn_output, _ = self.attn_layer(patch_embeddings, patch_embeddings, patch_embeddings)  # Shape: (batch_size, num_patches, hidden_dim)
+        attn_output = self.attn_norm(attn_output)  # Normalize After Self-Attention
+        # Add Positional Encoding Again Before Cross-Attention
+        attn_output = self.positional_encoding(attn_output)  # Add Positional Encoding Again Before Cross-Attention
 
-        # Apply Query-Based Attention Pooling (which now includes Positional Encoding)
-        # vision_features = self.attention_pooling(attn_output)  # Shape: (batch_size, hidden_dim)
-        # vision_features = self.norm(vision_features)  # Normalize after attention
+        # Process State Inputs
+        state_inputs = torch.cat([goal_distance, heading_error, velocity, omega, last_action, preference_ranking], dim=-1)  # (batch_size, 25, 8)
+        state_embedding = self.state_mlp(state_inputs)  # Shape: (batch_size, 25, hidden_dim)
+        # state_embedding = self.state_norm(state_embedding) # Norm to normalize before fusion
 
-        # Process numerical inputs
-        state_inputs = torch.cat([goal_distance, heading_error, velocity, past_action, current_action], dim=-1)  # (batch_size, 8)
-        state_embedding = self.state_mlp(state_inputs)  # Shape: (batch_size, 256)
-        state_embedding = self.state_norm(state_embedding) # Norm to normalize before fusion
-        state_embedding = state_embedding.unsqueeze(1)
+        # Generate Multiple Queries
+        query_list = [proj(state_embedding) for proj in self.state_query_proj]
+        state_queries = torch.stack(query_list, dim=2)  # (batch_size, 25, Q, hidden_dim)
 
-        fused_features, _ = self.cross_attention(state_embedding, attn_output, attn_output) # Shape: (batch_size, 1, hidden_dim)
-        fused_features = fused_features.squeeze(1)
-        # Predict reward
-        reward = self.reward_head(fused_features)  # (batch_size, 1)
 
-        return reward
\ No newline at end of file
+        # Reshape to match MultiheadAttention expected shape (batch_size, seq_length, hidden_dim)
+        state_queries = state_queries.view(batch_size * 25, self.num_queries, -1)  # (batch_size * 25, num_queries, hidden_dim)
+        attn_output = attn_output.unsqueeze(1).expand(-1, 25, -1, -1)  # Shape: (batch_size, 25, num_patches, hidden_dim)
+        attn_output = attn_output.reshape(batch_size * 25, attn_output.shape[2], attn_output.shape[3])  # Shape: (batch_size * 25, num_patches, hidden_dim)
+        
+        fused_features, _ = self.cross_attention(state_queries, attn_output, attn_output)
+        fused_features = self.cross_attention_norm(fused_features)
+        # Cross-Attention (Querying vision features with action-specific queries)
+        for i in range(self.num_self_attention_layers):
+            # Choose Queries, Keys, and Values. Example setup
+            residual = fused_features
+            fused_features, _ = self.self_attention_layers[i](fused_features, fused_features, fused_features)
+            
+            # fused_features = self.attention_norms[i+1](fused_features)
+            fused_features = fused_features + residual
+            fused_features = self.attention_norms[i](fused_features)
+
+        # Reshape fused features back to (batch_size, 25, num_queries, hidden_dim)
+        fused_features = fused_features.view(batch_size, 25, self.num_queries, -1)
+        fused_features = fused_features.reshape(batch_size, 25, -1)  # (batch_size, 25, hidden_dim)
+
+        # **MLP-based Query Fusion**
+        fused_features = fused_features.view(batch_size, 25, -1) # Shape: (batch_size, 25, num_queries * hidden_dim)
+        fused_features = self.query_fusion_mlp(fused_features)   # Shape: (batch_size, 25, hidden_dim)
+
+        fused_features = self.fusion_norm(fused_features)  # Normalize After Feature Fusion
+
+        # Predict rewards for all 25 actions
+        rewards = self.reward_head(fused_features).squeeze(-1)  # (batch_size, 25)
+
+        return rewards
\ No newline at end of file
