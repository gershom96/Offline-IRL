diff --git a/src/training/train_rm_scand.py b/src/training/train_rm_scand.py
index 04dcef7..e0a6433 100644
--- a/src/training/train_rm_scand.py
+++ b/src/training/train_rm_scand.py
@@ -26,7 +26,7 @@ HIDDEN_DIM = 768
 N_EPOCHS = 10
 train_val_split = 0.8
 num_workers = 4
-batch_print_freq = 10
+batch_print_freq = 5
 gradient_log_freq = 100
 notes = "implementing wandb"
 use_wandb = True
@@ -77,7 +77,7 @@ writer.add_text(
 )
 
 # Define Model, Loss, Optimizer
-model = RewardModelSCAND(num_queries=NUM_QUERIES, hidden_dim=HIDDEN_DIM).to(device)
+model = RewardModelSCAND(num_queries=NUM_QUERIES).to(device)
 criterion = PL_Loss()
 optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
 scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
@@ -123,7 +123,7 @@ for epoch in range(N_EPOCHS):
 
         if batch_count % batch_print_freq == 0:  # Log every 10 batches
             SPS = global_step / (time.time() - start_time)
-            print(f"Epoch [{epoch+1}/{N_EPOCHS}] | Batch {batch_count} | Train Loss: {loss.item():.4f}, steps per second: {SPS:.3f}")
+            print(f"Epoch [{epoch+1}/{N_EPOCHS}] | Batch {batch_count} | Train Loss: {loss.item():.4f}, steps per second: {SPS:.3f} | LR: {optimizer.param_groups[0]['lr']}")
             writer.add_scalar("charts/SPS", SPS, global_step)
             writer.add_scalar("epoch", epoch, global_step)
 
diff --git a/src/utils/__pycache__/reward_model_scand.cpython-310.pyc b/src/utils/__pycache__/reward_model_scand.cpython-310.pyc
index 377cdbf..4077cdd 100644
Binary files a/src/utils/__pycache__/reward_model_scand.cpython-310.pyc and b/src/utils/__pycache__/reward_model_scand.cpython-310.pyc differ
diff --git a/src/utils/reward_model_scand.py b/src/utils/reward_model_scand.py
index f4f8620..f65b4ea 100644
--- a/src/utils/reward_model_scand.py
+++ b/src/utils/reward_model_scand.py
@@ -97,7 +97,7 @@ class RewardModelSCAND(nn.Module):
             nn.Linear(256, 128),
             nn.GELU(),
             nn.Linear(128, 1),
-            nn.Tanh()  # Normalize output range
+            # nn.Tanh()  # Normalize output range
         )
 
         self._initialize_weights()
